# Agent Prompting & Agentic Workflows: Research Summary 2026

Comprehensive research findings on production-ready AI agents, assembled January 2026.

---

## 1. Agentic Workflows: Design Patterns & Best Practices

### Core Definition

**Agentic workflow**: A system using AI to take initiatives, make decisions, and exert control at various stages of a process.

**Key distinction:**
- **Deterministic agentic workflows**: Predefined code paths with LLM-enhanced steps
- **True agents**: LLMs dynamically decide next moves

### Nine Core Best Practices for Production (2026)

1. **Tool-first design** over Model Context Protocol (MCP)
2. **Pure-function invocation** for stability and debuggability
3. **Single-tool and single-responsibility agents** for clarity
4. **Externalized prompt management**
5. **Responsible-AI-aligned model-consortium design**
6. **Clean separation** between workflow logic and MCP servers
7. **Containerized deployment** for scalability
8. **KISS principle** (Keep it Simple, Stupid) for robustness
9. **Multi-agent consensus** for reliability

### When to Use What

| Use Case | Type | Characteristics |
|----------|------|-----------------|
| **Agentic Workflows** | Narrowly scoped, predictable tasks | Fixed runtimes, lower costs |
| **True Agents** | Open-ended tasks | Dynamic decision-making, creative outcomes |
| **Hybrid** | Most real-world cases | Combines deterministic code + workflows + agents |

**Source:** [Practical Guide for Designing Agentic AI Workflows](https://www.emergentmind.com/papers/2512.08769)

---

## 2. Multi-Agent Orchestration Patterns

### Market Context

- **Market size**: $5.25B (2024) → $52.62B (2030), 46.3% CAGR
- **Enterprise adoption**: 80% plan multi-agent orchestration within 2 years
- **Current state**: <10% successfully implemented

### Four Primary Orchestration Patterns

#### 1. Hub-and-Spoke (Centralized)

**Structure:** Central orchestrator manages all agent interactions

**Pros:**
- Predictable workflows
- Strong consistency
- Simplified debugging
- Good for compliance (finance, healthcare)

**Cons:**
- Central bottleneck
- Single point of failure

#### 2. Sequential (Pipeline)

**Structure:** Clear stage dependencies between agents

```
retrieve → analyze → draft → review
```

**Pros:**
- Simple execution
- Clear dependencies

**Cons:**
- Higher end-to-end latency
- No parallelization

#### 3. Supervisor-Worker

**Structure:** LLM supervisor directs specialized worker agents

**Use case:** Tasks that branch or require oversight

#### 4. Mesh (Distributed)

**Structure:** Decentralized control, agents communicate directly

**Pros:**
- Greater scalability
- No single bottleneck

**Cons:**
- More complex debugging
- Harder to reason about

### Performance Benefits

Compared to single-agent systems:
- **45% reduction** in hand-offs
- **3x faster** decision speed
- **45% faster** problem resolution
- **60% more accurate** outcomes

### Communication Protocols (2026)

Four major standards have emerged:
1. **MCP (Model Context Protocol)** - Anthropic's "USB-C for AI"
2. **ACP (Agent Communication Protocol)**
3. **A2A (Agent-to-Agent Protocol)** - Google, backed by 50+ companies including Microsoft, Salesforce
4. **ANP (Agent Network Protocol)**

**Source:** [Mastering Multi-Agent Orchestration](https://onabout.ai/p/mastering-multi-agent-orchestration-architectures-patterns-roi-benchmarks-for-2025-2026)

---

## 3. Tool Calling Optimization (2026)

### Advanced Techniques

#### ToolGen Paradigm (ICLR 2025)

**Innovation:** Integrate tool knowledge directly into LLM parameters

**Approach:**
- Represent each tool as a unique token
- Generate tool calls as part of next token prediction
- No additional retrieval steps needed

**Benefits:**
- Access to 47,000+ tools
- Enhanced performance and scalability
- No external retrieval latency

#### Parallel Function Calling Compiler

**Innovation:** Fuse similar tool operations under single functions at runtime

**Results:**
- **4x more parallel calls** than existing methods
- **40% reduction** in token costs
- **12% reduction** in latency

**Inspiration:** Hardware multiply-add fusion principles

### Key Trend

Optimization is moving from context-based tool retrieval → generative approaches with integrated tool knowledge + compiler-level parallelization

**Sources:**
- [ToolGen: Integrating Tool Knowledge into LLMs](https://arxiv.org/abs/2410.03439)
- [Parallel Function Calling Compiler](https://arxiv.org/abs/2405.17438)

---

## 4. Prompt Caching Strategies (2026)

### Performance Impact

**Cost & Latency Benefits:**
- **45-80% reduction** in API costs
- **13-31% improvement** in time to first token (TTFT)
- **50% discount** on cached input tokens (OpenAI)

### Automatic Prefix Caching (APC)

**Mechanism:** Reuse key-value (KV) cache from previous queries when new queries share the same prefix

**Best for:**
- Long document queries
- Multi-turn conversations
- Repeated system prompts

### Strategic Cache Control (2026 Research)

**Key finding:** Naive full-context caching can paradoxically **increase latency**

**Optimal strategy:**
```
✅ Position dynamic content at END of system prompt
✅ Exclude dynamic tool results from cache
✅ Keep static instructions cached
```

### Context Window Expansion

**Gemini 3.0:** 1-million-token context with >99% retrieval accuracy

**Use cases:**
- Large document processing
- Full codebase edits
- Extended multi-turn conversations

### OpenAI Implementation Details

- Automatic discount on inputs >1,024 tokens
- Cache clears after 5-10 minutes of inactivity
- Works with prompt caching API

**Sources:**
- [OpenAI Prompt Caching](https://openai.com/index/api-prompt-caching)
- [Strategic Cache Control Evaluation](https://arxiv.org/abs/2601.06007)

---

## 5. Agent Evaluation & Reliability

### Critical Gap Identified

**Problem:** Traditional benchmarks focus on task success, ignoring:
- Process quality
- Reasoning transparency
- Failure recovery
- Cost-efficiency

### Three Major Frameworks (2026)

#### 1. HB-Eval (System-Level Reliability)

**Three complementary metrics:**

1. **Failure Resilience Rate** - Recovery from systematic faults
2. **Planning Efficiency Index** - Trajectory optimality
3. **Traceability Index** - Reasoning transparency

**Key finding:** 42.9 percentage point gap between nominal success rates and stressed performance

**Best result:** Integrated resilience architecture achieved 94.2% ± 2.1% failure recovery rates

#### 2. Agent-Pex (Microsoft Research)

**Capabilities:**
- Extracts behavioral specifications from agent prompts
- Assesses compliance across multiple dimensions:
  - Argument validity
  - Output compliance
  - Plan sufficiency
- Enables comparison across different architectures
- Scalable to thousands of traces

#### 3. Google's Three-Pillar Framework

**Evaluation dimensions:**

1. **Agent success/quality** - Final output assessment
2. **Process and trajectory analysis** - Internal decision-making
3. **Interaction assessment** - How agent collaborates

**Critical innovation:** Addresses "silent failures" (correct outputs through flawed processes)

### Benchmark Leaderboards (January 2026)

#### GAIA (General AI Assistants)

**Top performers:**
- JoinAI_V2.2: 90.7% average score
- NVIDIA agents: 90.37%
- JD Enterprise Intelligence: 90.37%

#### HAL (Holistic Agent Leaderboard)

**Spans 9 benchmarks:** Coding, web navigation, science, customer service

**Top performers:**
- **AssistantBench**: Browser-Use with o3 Medium (38.8%, $15.15)
- **CORE-Bench Hard**: CORE-Agent with Claude Opus 4.1 (51.1%, $412.42)
- **GAIA**: HAL Generalist Agent with Claude Sonnet 4.5 (74.5%, $178.20)
- **SWE-bench**: SWE-Agent with Claude Sonnet 4.5 High (72.0%, $463.90)

**Key insights:**
- Higher reasoning effort actually reduced accuracy in most runs
- Agents sometimes exploit benchmark shortcuts
- Shift needed: benchmark performance → real-world reliability

**Evaluation costs:** $40,000 for 21,730+ agent rollouts

**Sources:**
- [HB-Eval Framework](https://labs.sciety.org/articles/by?article_doi=10.20944%2Fpreprints202512.2186.v1)
- [Agent-Pex](https://www.microsoft.com/en-us/research/project/agent-pex-automated-evaluation-and-testing-of-ai-agents/)
- [HAL Leaderboard](https://hal.cs.princeton.edu/)

---

## 6. Claude Computer Use & Autonomous Agents

### Core Capabilities

**Computer Use Tool** enables Claude to:
- Capture screenshots and perceive visual interfaces
- Understand UI elements
- Control mouse clicks and keyboard input

### Three-Function Autonomy

1. **Thinking** - Claude Opus 4.5 with 200K context window
2. **Seeing** - Screenshot capture via Computer Use API
3. **Acting** - Mouse, keyboard, shell commands in isolated environments

### Agent SDK Tools

Anthropic provides tools for autonomous agents:
- Computer use tool
- Bash execution
- Code execution
- Web search
- File operations

### Practical Applications

**Current use cases:**
- File organization and bulk document processing
- Data extraction from PDFs/images
- Spreadsheet automation and data analysis
- Meeting summaries and report generation
- Expense tracking and receipt management

### Safety Features

- Folder-level permissions
- Isolated VM environments
- Approval gates for sensitive operations

**Sources:**
- [Claude Agent SDK](https://platform.claude.com/docs/en/agent-sdk/overview)
- [Computer Use Tool](https://platform.claude.com/docs/en/agents-and-tools/tool-use/computer-use-tool)

---

## 7. Model Context Protocol (MCP): Best Practices

### Core Principle

MCP is an **open standard** that acts as a "USB-C port for AI applications" - standardized connection between AI models and data sources/tools.

### Design Principles for Agent Integration

1. **Single responsibility** - One clear domain and auth boundary per server
2. **Bounded toolsets** - Focused tools with specific contracts; avoid "kitchen-sink" servers
3. **Contracts first** - Strict input/output schemas, explicit side effects, documented errors
4. **Stateless by default** - Keep execution stateless for scale and resiliency
5. **Security by design** - Integrate identity, authorization, audit into server
6. **Controlled autonomy** - Least-privilege tools with approval paths for high-impact actions

### Transport Options

**stdio:**
- Best for: Local, per-user integrations
- Pros: Strong isolation
- Cons: Limited to local processes

**Streamable HTTP:**
- Best for: Remote, shared services
- Pros: Scale-out scenarios
- Cons: Network overhead

### MCP Gateway Pattern (Enterprise)

**Use case:** Centralized security across many servers and tenants

**Responsibilities:**
- Centralized authentication/authorization
- Routing
- Rate limiting
- Policy-as-code enforcement
- Multi-tenancy support

### Development Focus

**From day one:**
- Observable, secure designs
- Least-privilege integrations
- Comprehensive testing (unit, integration, E2E)
- Container-based packaging
- Supply chain integrity (SBOMs, signing, provenance)
- Production SLOs and monitoring
- Policy-as-code for tool governance

**Sources:**
- [MCP Best Practices](https://mcp-best-practice.github.io/mcp-best-practice/)
- [MCP Architecture Guide](https://modelcontextprotocol.info/docs/best-practices/)

---

## 8. Agent Memory Systems (2026)

### Evolution Beyond RAG

**Traditional RAG limitations:**
- Treats memory as stateless lookup table
- Read-only retrieval
- No temporal continuity
- Cannot accumulate, mutate, or disambiguate over long horizons

### Three Advanced Memory Systems

#### 1. Continuum Memory Architecture (CMA)

**Key features:**
- Maintains internal state across interactions
- Persistent storage
- Selective retention
- Associative routing
- Temporal chaining
- Consolidation into higher-order abstractions

**Innovation:** Addresses RAG's structural inability to handle long-horizon memory

#### 2. EverMemOS (Self-Organizing Memory OS)

**Engram-inspired lifecycle:**

1. **MemCells** - Capture episodic traces and atomic facts from dialogue
2. **MemScenes** - Semantic consolidation of MemCells
3. **Guided retrieval** - For reasoning tasks

**Results:** Significant improvements over SOTA on memory-augmented reasoning benchmarks

#### 3. Mem0 (Production-Ready)

**Architecture:**
- Dynamically extracts, consolidates, retrieves information
- Graph-based memory representations

**Performance:**
- **26% relative improvement** over baselines
- **91% lower latency** vs full-context approaches
- **>90% token cost savings**

### Key Challenge (2026)

Existing systems treat memory as flat, isolated records rather than consolidated, coherent knowledge structures needed for long-horizon reasoning and consistency.

**Sources:**
- [Continuum Memory Architecture](https://arxiv.org/abs/2601.09913)
- [EverMemOS](https://www.arxiv.org/pdf/2601.02163)
- [Mem0](https://arxiv.org/html/2504.19413v1)

---

## 9. Agent Security & Guardrails (2026)

### Unique Agent Security Risks

**Beyond traditional LLM threats:**

1. **Prompt Injection** - Malicious instructions via user input or external data
2. **Tool Abuse & Privilege Escalation** - Overly permissive tools exploited
3. **Data Exfiltration** - Sensitive info leaked through tool calls/outputs
4. **Memory Poisoning** - Malicious data persisted in agent memory
5. **Goal Hijacking** - Manipulating agent objectives
6. **Excessive Autonomy** - High-impact actions without human oversight
7. **Denial of Wallet** - Unbounded agent loops causing excessive API costs

### Five Core Safety Patterns

#### 1. Tool Security & Least Privilege

**Best practices:**
- Grant minimum required tools
- Per-tool permission scoping
- Allowlists for specific operations
- Block sensitive files (`.env`, `.key`, `.pem`)

#### 2. Input/Output Guardrails

**Implementation:**
- Callbacks to validate model/tool calls before/after execution
- Built-in safety features (content filters, system instructions)
- Additional safety layers using lightweight screening models

#### 3. Identity & Authorization

**Control mechanisms:**
- Agent authentication (who the agent acts as)
- User authentication (who invoked the agent)
- Different tool sets for different trust levels

#### 4. Sandboxing & Environment Controls

**Protection layers:**
- Sandboxed execution for model-generated code
- VPC-SC network controls
- Isolated environments

#### 5. Monitoring & Enforcement

**Continuous oversight:**
- Control plane monitoring
- Data plane monitoring
- Access monitoring
- Response filters to detect security violations

### Current State

**Critical gap:** Most AI agents in production today lack proper safety measures.

**Sources:**
- [OWASP AI Agent Security Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/AI_Agent_Security_Cheat_Sheet.html)
- [Google ADK Safety Guide](https://google.github.io/adk-docs/safety/)

---

## 10. Agent Debugging & Observability

### Deep Agent Debugging Challenges

**Key differences from simple LLM apps:**
- Run for minutes across dozens/hundreds of steps
- Multiple user interactions (human-in-the-loop)
- Massive traces difficult to analyze manually
- Longer prompts (hundreds/thousands of lines)

### LangSmith Observability Framework

**Data hierarchy:**

```
Threads (full conversations)
  └─ Traces (single execution)
      └─ Runs (individual steps: LLM calls, tool calls)
```

**Capabilities:**
- Complete visibility into agent behavior
- Real-time monitoring
- Alerting
- Usage insights

### New Tools (2025-2026)

#### Polly

**What:** AI assistant for agent engineering within LangSmith

**Purpose:** Help analyze complex traces

#### langsmith-fetch

**What:** CLI tool for debugging

**Integration:** Equips coding agents (Claude Code, DeepAgents CLI) with debugging capabilities

### Framework Support

Integrations with:
- LangGraph
- Anthropic
- OpenAI Agents SDK
- CrewAI
- Others

**Source:** [LangSmith Debugging](https://blog.langchain.com/debugging-deep-agents-with-langsmith/)

---

## 11. Cost Optimization Strategies (2026)

### The Core Problem

**Token costs don't explode from dramatic mistakes** - they accumulate quietly through architectural inefficiencies.

**Key inefficiencies:**
- Dragging full conversation histories forward
- Redundant context transfers between agents
- Failing to distinguish ephemeral interactions from durable state

### Seven Optimization Strategies

#### 1. Context Management

**Design explicit lifecycle rules** for context

**Tactics:**
- Conversation truncation (remove outdated info)
- Context compression (replace raw histories with summaries)
- Pass summaries between agents, not complete analysis

#### 2. Summarization Checkpoints

**Approach:** Periodically collapse recent interactions into task-relevant summaries

**Keep:**
- Decisions
- Constraints

**Discard:**
- Small talk
- Failed attempts

#### 3. Prompt Optimization

**Actions:**
- Strip redundant instructions
- Combine related queries
- Remove verbose examples from system prompts

**Strategic thinking:** Optimize token flow across entire agent architecture, not prompts in isolation

#### 4. Model & Provider Selection

**Cost comparison (per million input tokens):**
- Gemini 2.5 Flash: $0.40
- Claude Sonnet 4.5: $3.00

**Note:** Output tokens remain more expensive than input tokens

#### 5. Multi-Agent Efficiency

**Problem:** Token usage multiplies through context passing

**Solution:** Minimize redundant transfers, design workflows where downstream agents receive only necessary information

#### 6. Prompt Caching

**Impact:**
- 45-80% cost reduction
- 13-31% latency improvement

#### 7. Smart Memory Systems

**Modern approaches:**
- Mem0: >90% token cost savings
- CMA: Selective retention vs full context

### Economic Reality

**Cost scaling example:**
```
Single user: $0.50/day
1,000 users: $15,000/month
10,000 users: $150,000/month
```

**Critical insight:** Token optimization is foundational design work, not a post-deployment fix.

**Sources:**
- [Reducing Token Costs in Long-Running Workflows](https://agentsarcade.com/blog/reducing-token-costs-long-running-agent-workflows)
- [Managing AI Agent Costs](https://mbrenndoerfer.com/writing/managing-reducing-ai-agent-costs-optimization-strategies)

---

## 12. Agent Framework Comparison (2026)

### Three Leading Frameworks

**Market adoption:** 80% of companies plan AI automation by 2025, with multi-agent systems at the forefront

**Enterprise impact:**
- 25%+ increase in automation efficiency
- ~30% cut in operational costs

### Framework Characteristics

#### LangGraph

**Architecture:** Graph-driven workflow with DAG (Directed Acyclic Graph) functions

**Strengths:**
- Granular control
- Easy-to-understand structure
- Seamless LangChain integration

**Weaknesses:**
- Requires well-defined state management upfront

**Best for:** Structured, predictable workflows

#### CrewAI

**Architecture:** Role-based collaborative agents

**Strengths:**
- Strong focus on accessibility
- Structured approach
- Easy to get started

**Innovation:** Brought role-playing and organization to multi-agent systems

**Best for:** Teams wanting structured collaboration

#### AutoGen

**Architecture:** Conversational and autonomous agent patterns

**Strengths:**
- Mature foundation
- Started the multi-agent conversation space
- Proven in production

**Best for:** Multi-agent collaboration with flexible communication

### Critical Evaluation Factors

When choosing a framework:

1. **State and memory management** - How does it handle long-term context?
2. **Human-in-the-loop** - Can humans intervene in workflows?
3. **Orchestration flexibility** - How much control over agent coordination?
4. **Setup vs production readiness** - Easy start vs robust deployment?
5. **Structured output** - How well does it enforce output schemas?
6. **Governance** - Does it support compliance and auditing?
7. **Ethical AI modules** - Now standard in 2026

**Sources:**
- [LangGraph vs CrewAI vs AutoGen (2026)](https://o-mega.ai/articles/langgraph-vs-crewai-vs-autogen-top-10-agent-frameworks-2026)
- [Production Engineer's Comparison](https://python.plainenglish.io/autogen-vs-langgraph-vs-crewai-a-production-engineers-honest-comparison-d557b3b9262c)

---

## Key Takeaways for Prompt Engineering (2026)

### 1. Agent Design is Architecture

- **Don't:** Treat agents as "smart prompts with tools"
- **Do:** Design complete systems with memory, orchestration, and governance

### 2. Production Requirements Have Evolved

**2024:** Focus on making agents work
**2026:** Focus on reliability, cost, security, observability

### 3. Multi-Agent is the New Default

- Single agents for simple tasks
- Multi-agent orchestration for complex workflows
- Hybrid approaches for most real-world cases

### 4. Memory is Critical

- RAG alone is insufficient
- Need: consolidation, temporal continuity, coherent knowledge structures
- Modern systems: CMA, EverMemOS, Mem0

### 5. Cost Optimization is Foundational

- Not a post-deployment fix
- Requires architectural thinking
- Key levers: context management, caching, model selection, memory systems

### 6. Security Cannot Be Afterthought

- 7 unique agent security risks
- 5 core safety patterns required
- Most production agents still lack proper safety measures

### 7. Evaluation Must Go Beyond Task Success

- Measure: process quality, reasoning transparency, failure recovery
- Use: HB-Eval, Agent-Pex, HAL-style comprehensive evaluation
- Address "silent failures"

### 8. Tool Calling is Evolving Fast

- 2024: Context-based retrieval
- 2026: Integrated tool knowledge, compiler-level parallelization
- 4x more parallel calls, 40% token reduction possible

### 9. Observability is Non-Negotiable

- Deep agents are too complex for manual analysis
- Need: structured tracing, AI debugging assistants
- LangSmith, Polly, framework integrations

### 10. Standards are Emerging

- MCP as "USB-C for AI"
- A2A backed by 50+ companies
- Follow best practices: single responsibility, bounded toolsets, contracts first

---

## Recommended Reading Order

**For beginners:**
1. Agentic Workflows (Section 1)
2. Tool Calling Optimization (Section 3)
3. Agent Framework Comparison (Section 12)

**For practitioners:**
1. Multi-Agent Orchestration (Section 2)
2. Cost Optimization (Section 11)
3. MCP Best Practices (Section 7)

**For production engineers:**
1. Agent Evaluation & Reliability (Section 5)
2. Security & Guardrails (Section 9)
3. Debugging & Observability (Section 10)

**For researchers:**
1. Memory Systems (Section 8)
2. Prompt Caching Strategies (Section 4)
3. Claude Computer Use (Section 6)

---

---

## 13. Claude Code: Anthropic's Production Architecture

Anthropic's Claude Code provides another production perspective on agentic systems, complementing OpenAI's Codex architecture.

### The Agentic Loop (Anthropic's Model)

**Three-phase cycle:**
```
1. Gather Context (search files, read code)
2. Take Action (edit files, run commands)
3. Verify Results (run tests, check output)
→ Repeat until task complete
```

**Key insight:** Phases blend together. Claude uses tools throughout, not in strict sequence.

**User participation:** You can interrupt at any point to steer, provide context, or change direction.

### Models vs Tools: Clear Separation

**Anthropic's architecture separates two concerns:**

1. **Models** - Reasoning layer
   - Understand code in any language
   - Figure out what needs to change
   - Break work into steps
   - Adjust based on learning

2. **Tools** - Action layer
   - File operations (read, edit, create, rename)
   - Search (find files, search content, explore)
   - Execution (shell commands, tests, git)
   - Web (search, fetch docs, error lookup)
   - Code intelligence (type errors, definitions, references)

**Agentic Harness:** Claude Code is the harness around the model - provides tools, context management, execution environment that turn LLM into capable agent.

### Context Management: Loading Strategies

**Key difference from OpenAI Codex:** Anthropic uses extension layering.

#### Extension Layer

| Feature | Loads | Context Cost | Use Case |
|---------|-------|--------------|----------|
| **CLAUDE.md** | Session start | Every request | Always-on rules, conventions |
| **Skills** | Descriptions at start, content on-demand | Low until used | Reference material, workflows |
| **MCP servers** | Session start | Every request | External service connections |
| **Subagents** | On spawn | Isolated context | Parallel work, context isolation |
| **Hooks** | On trigger | Zero (external) | Automation, side effects |

**Loading optimization:**
- Skills load descriptions first (~few hundred tokens), full content only when invoked
- User-only skills (`disable-model-invocation: true`) have zero cost until invoked
- Subagents get fresh context, don't inherit conversation history
- Hooks run externally, don't consume context

#### Context Filling Strategy

**When context approaches limit:**
1. Clear older tool outputs first
2. Summarize conversation if needed
3. Preserve: user requests, key code snippets
4. May lose: detailed instructions from early conversation

**Anthropic's recommendation:** Put persistent rules in CLAUDE.md, not conversation history.

**Compaction control:**
- Add "Compact Instructions" section to CLAUDE.md
- Use `/compact focus on [topic]` to specify what to preserve

### Sessions: Ephemeral by Design

**Critical insight:** Claude Code has **no persistent memory between sessions**.

**Each new session starts fresh:**
- No learning preferences over time
- Doesn't remember last week's work
- To persist knowledge: put in CLAUDE.md

**Session-specific features:**
- Checkpoints (undo file changes within session)
- Resume (`--continue`) - same session ID
- Fork (`--fork-session`) - new session ID with history

**Sessions tied to directories**, not git branches:
- Switch branches → same conversation, different files
- Use git worktrees for parallel sessions in different directories

### Safety Mechanisms

**Two layers:**

1. **Checkpoints** - Every file edit snapshots current contents
   - Reversible within session
   - Separate from git
   - Only cover file changes (not external actions)

2. **Permission Modes** (Shift+Tab to cycle)
   - **Default**: Ask before edits and commands
   - **Auto-accept edits**: Edits without asking, still asks for commands
   - **Plan mode**: Read-only tools, create plan before execution

**Allowlists:** Specify trusted commands in `.claude/settings.json` (e.g., `npm test`, `git status`)

### Extension Layering: Production Patterns

**How features combine:**

| Pattern | Implementation | Example |
|---------|---------------|---------|
| **Skill + MCP** | MCP provides connection, skill teaches usage | MCP connects DB, skill documents schema |
| **Skill + Subagent** | Skill spawns isolated workers | `/review` kicks off security, perf, style subagents |
| **CLAUDE.md + Skills** | CLAUDE.md = always-on, Skills = on-demand | CLAUDE.md says "follow conventions", skill has full guide |
| **Hook + MCP** | Hook triggers external actions via MCP | Post-edit hook sends Slack notification |

**Feature precedence when same name exists:**
- **CLAUDE.md**: Additive (all levels contribute)
- **Skills/Subagents**: Override by priority (managed > user > project)
- **MCP servers**: Override by scope (local > project > user)
- **Hooks**: Merge (all fire for matching events)

### Context Cost Optimization

**Cost by feature (per request):**

```
High cost (every request):
- CLAUDE.md: Full content
- MCP servers: All tool definitions

Medium cost (descriptions only):
- Skills: Descriptions at start, full content on invocation

Zero cost (until used):
- Skills with disable-model-invocation: true
- Subagents (isolated context)
- Hooks (run externally)
```

**Optimization strategies:**

1. **Keep CLAUDE.md under ~500 lines**
   - Move reference material to skills
   - Skills load on-demand

2. **Use disable-model-invocation: true**
   - For skills you invoke manually
   - Saves context, ensures only you trigger

3. **Disconnect unused MCP servers**
   - Check cost: `/mcp`
   - Each server adds tool definitions

4. **Use subagents for isolation**
   - Work doesn't bloat main session
   - Returns only summary

5. **MCP Tool Search (default enabled)**
   - Loads MCP tools up to 10% of context
   - Defers rest until needed

### Best Practices (Anthropic's Recommendations)

**1. It's a conversation**
```
❌ Don't expect perfect prompts first try

✅ Start, then refine:
"Fix the login bug"
[Claude investigates]
"That's not quite right. The issue is in session handling."
[Claude adjusts]
```

**2. Be specific upfront**
```
❌ Vague: "fix the login bug"

✅ Specific:
"The checkout flow is broken for users with expired cards.
Check src/payments/ for the issue, especially token refresh.
Write a failing test first, then fix it."
```

**3. Give Claude something to verify against**
```
✅ "Implement validateEmail. Test cases:
- 'user@example.com' → true
- 'invalid' → false
- 'user@.com' → false
Run tests after."
```

**4. Explore before implementing**
```
Use plan mode (Shift+Tab twice):

"Read src/auth/ and understand how we handle sessions.
Then create a plan for adding OAuth support."

[Review plan, refine through conversation]
[Then let Claude implement]
```

**5. Delegate, don't dictate**
```
❌ "Read src/auth/session.js, then read src/auth/token.js, then..."

✅ "The checkout flow is broken for users with expired cards.
The relevant code is in src/payments/. Can you investigate and fix it?"
```

**6. Interrupt and steer**
- Can interrupt at any point
- Type correction and press Enter
- Claude stops and adjusts
- No need to wait or start over

### Skills: Reference vs Action

**Two skill types:**

1. **Reference skills** - Knowledge Claude uses throughout session
   ```markdown
   # API Style Guide
   
   Endpoint patterns:
   - GET /api/{version}/{resource}
   - POST /api/{version}/{resource}
   ...
   ```

2. **Action skills** - Workflows triggered with `/<name>`
   ```markdown
   ---
   name: deploy
   description: Deploy to production
   ---
   
   1. Run tests
   2. Build artifacts
   3. Push to registry
   4. Update k8s deployment
   ```

**Skills in subagents:**
- Skills passed to subagent are **fully preloaded** at launch
- Different from main session's on-demand loading
- Subagents don't inherit skills - must specify explicitly

### Subagents: Context Isolation

**When subagent spawns, it gets:**
- System prompt (shared with parent for cache efficiency)
- Full content of skills listed in `skills:` field
- CLAUDE.md and git status (inherited)
- Context passed in prompt

**Doesn't inherit:**
- Conversation history
- Invoked skills from parent
- Session-scoped permissions

**Use cases:**
- Tasks that read many files (only summary returns)
- Parallel work
- Context window near full
- Specialized workers with specific skills

### Comparison: Anthropic vs OpenAI Approaches

| Aspect | Claude Code (Anthropic) | Codex CLI (OpenAI) |
|--------|------------------------|-------------------|
| **Extension Model** | Layered (CLAUDE.md, Skills, MCP, Subagents, Hooks) | Prompt-centric with tools |
| **Context Strategy** | On-demand loading (Skills), isolated workers (Subagents) | Stateless requests, compaction |
| **Memory Persistence** | Ephemeral sessions, CLAUDE.md for persistence | Previous_response_id optional, ZDR compatible |
| **Safety Model** | Checkpoints + permission modes | Sandbox configuration + approval gates |
| **Tool Organization** | Extension layer on top of core loop | Flat tool list in JSON |
| **Caching Approach** | Extension layering optimizes cache hits | Static first, dynamic last |

**Key philosophical differences:**

**Anthropic (Claude Code):**
- Emphasizes extension ecosystem
- Clear separation: always-on (CLAUDE.md) vs on-demand (Skills)
- Context isolation via subagents
- Rich permission model (modes + allowlists)

**OpenAI (Codex):**
- Emphasizes prompt structure
- Role hierarchy (system > developer > user > assistant)
- Context management via compaction
- Sandbox-based permissions

**Both converge on:**
- Agentic loop architecture
- Tool calling as core capability
- Prompt caching critical for performance
- Stateless execution preferred
- Context window management essential

### Production Patterns from Claude Code

**1. CLAUDE.md Structure**
```markdown
# Project: My App

## Conventions
- Use pnpm, not npm
- Run tests before committing
- Follow API patterns in /docs/api-guide.md

## Build Commands
- Dev: pnpm dev
- Test: pnpm test
- Build: pnpm build

## "Always Do" Rules
- Add JSDoc for public APIs
- Update tests when changing behavior
- Never modify generated files in /dist
```

**2. Skill Organization**
```
.claude/
  skills/
    reference/        # Knowledge (API docs, patterns)
      api-guide.md
      data-model.md
    workflows/        # Invocable actions
      deploy.md       # Trigger with /deploy
      review.md       # Trigger with /review
      release.md      # Trigger with /release
```

**3. MCP Integration Pattern**
```
1. MCP server provides connection (e.g., database)
2. Skill documents how to use it:
   - Schema
   - Query patterns
   - Common operations
3. CLAUDE.md references both:
   "Use /db:query for database access. Follow patterns in data-model skill."
```

**4. Subagent Delegation**
```
Main session: Coordinate high-level work
  ↓
Spawn subagents for:
  - Codebase analysis (reads 100+ files, returns summary)
  - Parallel implementations (feature A, B, C independently)
  - Specialized tasks (security review with security-focused skills)
```

**5. Hook Automation**
```json
{
  "hooks": {
    "after-edit": "npx eslint --fix {file}",
    "before-commit": "pnpm test",
    "after-session-start": "./scripts/check-env.sh"
  }
}
```

### Key Metrics to Track

**From Claude Code production:**

| Metric | Target | Indicates |
|--------|--------|-----------|
| **CLAUDE.md size** | <500 lines | Context efficiency |
| **Skills loaded** | <5 active | Context optimization |
| **MCP servers** | <3 connected | Tool definition cost |
| **Session length** | <50 turns | Before compaction needed |
| **Context usage** | <60% of limit | Headroom for work |

**Red flags:**
- CLAUDE.md >1000 lines (move to skills)
- >5 MCP servers connected (disconnect unused)
- >80% context usage (spawn subagent or compact)
- Skills not triggering (improve descriptions)

**Sources:**
- [Claude Code: How It Works](https://code.claude.com/docs/how-claude-code-works)
- [Claude Code: Extend Features](https://code.claude.com/docs/features-overview)

---

## 14. OpenManus: MetaGPT's Modular Agent Framework

OpenManus is an open-source framework from the MetaGPT community that demonstrates another production approach to building general-purpose AI agents.

### Core Architecture: Four-Layer Design

**1. Agent Layer (Hierarchical Structure)**

```
Base: ToolCallAgent
  ↓
PlanningAgent (task decomposition)
  ↓
ReActAgent (think-act-observe)
  ↓
Manus (main agent, user entry point)
```

**Key insight:** Progressive specialization through inheritance. Each layer adds capabilities:
- **ToolCallAgent**: Base class for tool/function calling
- **PlanningAgent**: Adds task execution plan creation/management
- **ReActAgent**: Implements think-act-observe loop pattern
- **Manus**: Main agent that orchestrates everything

**2. Tool Layer (External Integration)**

Standard tools provided:
- **Execution**: PythonExecute, Bash
- **Web**: BrowserUseTool, GoogleSearch, WebSearch
- **Files**: FileSaver, StrReplaceEditor
- **Planning**: PlanningTool, CreateChatCompletion
- **Control**: Terminate

**3. Prompt Layer (Interaction Patterns)**

Three prompt types define agent behavior:

| Prompt Type | Purpose | Example Use |
|-------------|---------|-------------|
| **System Prompts** | Define agent role and capability scope | "You are an expert software architect" |
| **Step Prompts** | Guide tool usage for specific steps | "Use BrowserUseTool to navigate to..." |
| **Planning Prompts** | Enable task decomposition | "Break this into subtasks with dependencies" |

**4. LLM Interaction Layer (Decision Engine)**

Handles communication with LLMs for decision-making and content generation.

### Supporting Components

**Memory Component:**
- Stores conversation history
- Manages context across turns
- Enables agent to reference past decisions

**Flow Component:**
- Graph-based workflow coordination
- Multi-agent collaboration
- Task routing and orchestration

### Think-Act-Observe Loop (ReActAgent)

**Pattern:**
```
1. Think: Reason about current state and next action
2. Act: Execute tool call or action
3. Observe: Process results and update understanding
→ Repeat until task complete
```

**Implementation:**
```python
class ReActAgent:
    def run(self, task):
        while not done:
            # Think
            thought = self.reason_about_state(current_state)
            
            # Act
            action = self.select_action(thought)
            result = self.execute_action(action)
            
            # Observe
            current_state = self.update_state(result)
            done = self.check_completion(current_state)
```

**Difference from other frameworks:**
- More structured than Codex's tool calling
- More explicit than Claude Code's extension layer
- Built-in planning vs external planning tools

### Modular Design Philosophy

**Key principle:** High extensibility through clear interfaces.

**Component separation:**
```
/agents/          # Agent implementations
/tools/           # Tool definitions
/sandbox/         # Isolated execution environments
/workflows/       # Workflow definitions
/prompts/         # Prompt templates
```

**Benefits:**
- Easy to add new tools without modifying agents
- Clear boundaries between concerns
- Reusable components across different agents

### Workflow Management: Graph-Based

**OpenManus uses directed graphs for workflows:**

```python
workflow = {
    "start": {
        "agent": "PlanningAgent",
        "next": "execute"
    },
    "execute": {
        "agent": "ReActAgent",
        "next": ["verify", "error_handler"]
    },
    "verify": {
        "agent": "ToolCallAgent",
        "next": "end"
    },
    "error_handler": {
        "agent": "ReActAgent",
        "next": "execute"
    }
}
```

**Advantages:**
- Visual representation of agent flow
- Easy to modify and extend
- Built-in error handling paths
- Supports loops and branches

### Prompt Engineering Patterns

**1. System Prompt Structure**

```markdown
# Role Definition
You are [specific role with expertise].

# Capabilities
You have access to the following tools:
- [tool 1]: [when to use]
- [tool 2]: [when to use]

# Constraints
- Always verify before making changes
- Ask for clarification when ambiguous
- Break complex tasks into steps

# Output Format
[Specify expected format]
```

**2. Step Prompt Pattern**

```markdown
Current Step: [step name]
Objective: [what to accomplish]

Available Tools:
- [relevant tools for this step]

Previous Context:
[summary of previous steps]

Instructions:
1. [specific instruction 1]
2. [specific instruction 2]

Output Requirements:
[expected output format]
```

**3. Planning Prompt Pattern**

```markdown
Task: [high-level task description]

Break this down into:
1. Subtasks with clear objectives
2. Dependencies between subtasks
3. Required tools for each subtask
4. Success criteria for each subtask
5. Estimated complexity (low/medium/high)

Format:
```json
{
  "subtasks": [
    {
      "id": "task-1",
      "objective": "...",
      "dependencies": [],
      "tools": ["tool1", "tool2"],
      "success_criteria": "...",
      "complexity": "medium"
    }
  ]
}
```
```

### Containerized Execution

**Docker-based isolation:**
- Each agent runs in isolated container
- Prevents side effects between agents
- Easy deployment and scaling
- Consistent environment across dev/prod

**Security benefits:**
- Limited access to host system
- Resource limits per container
- Network isolation
- Audit trail through container logs

### Multi-Agent Collaboration Patterns

**1. Sequential Pipeline**
```
Agent A → Agent B → Agent C
(planning) (execution) (verification)
```

**2. Parallel Execution**
```
       Agent B (security review)
      /
Agent A → Agent C (performance review)
      \
       Agent D (code quality)
```

**3. Hierarchical Delegation**
```
Manus (coordinator)
  ↓
PlanningAgent (breaks down task)
  ↓ ↓ ↓
ReActAgent₁  ReActAgent₂  ReActAgent₃
(parallel workers)
```

### Key Differences from Other Frameworks

| Aspect | OpenManus | Codex (OpenAI) | Claude Code (Anthropic) |
|--------|-----------|----------------|------------------------|
| **Architecture** | Hierarchical agents | Flat tools | Extension layers |
| **Prompts** | Three types (system, step, planning) | Role hierarchy | CLAUDE.md + Skills |
| **Workflow** | Graph-based | Agentic loop | Phases (gather, act, verify) |
| **Planning** | Built-in PlanningAgent | Planning tool | Plan mode |
| **Execution** | Docker containers | Sandbox | Checkpoints |
| **Collaboration** | Graph coordination | Not emphasized | Subagents |

### Best Practices from OpenManus

**1. Agent Hierarchy Design**

```
✅ Good: Progressive specialization
Base (tool calling) → Planning → ReAct → Specialized

❌ Bad: Flat structure with all capabilities
MonolithAgent (everything in one class)
```

**2. Prompt Type Separation**

```
✅ Good: Separate prompts by purpose
- System: Role and capabilities
- Step: Current action guidance
- Planning: Task decomposition

❌ Bad: Single giant prompt
"You are X and you should do Y and when you see Z..."
```

**3. Graph-Based Workflows**

```
✅ Good: Explicit flow with error handling
start → plan → execute → verify → end
              ↓ (if error)
           error_handler → retry

❌ Bad: Implicit flow
"Just keep trying until it works"
```

**4. Tool Organization**

```
✅ Good: Categorized by domain
/tools/
  execution/  (bash, python)
  web/        (browser, search)
  files/      (read, write, edit)
  planning/   (plan, decompose)

❌ Bad: Flat list
All tools in one file
```

### Integration with Existing Patterns

**OpenManus patterns complement existing approaches:**

**1. Combine with Codex prompt caching:**
- System prompts (static, cacheable)
- Step prompts (dynamic, per-action)
- Planning prompts (occasional, complex)

**2. Combine with Claude Code extensions:**
- CLAUDE.md = System prompts
- Skills = Step prompt templates
- Subagents = ReActAgent instances

**3. Combine with multi-agent orchestration:**
- OpenManus graphs = Hub-and-spoke coordinator
- ReActAgent = Worker agents
- Manus = Supervisor

### Production Patterns

**1. Task Decomposition**

```python
# OpenManus Planning Pattern
task = "Build a REST API"

plan = PlanningAgent.decompose(task)
# Returns:
[
  {"step": "Design schema", "agent": "ToolCallAgent", "tools": ["FileSaver"]},
  {"step": "Generate code", "agent": "ReActAgent", "tools": ["PythonExecute"]},
  {"step": "Write tests", "agent": "ReActAgent", "tools": ["PythonExecute"]},
  {"step": "Run tests", "agent": "ToolCallAgent", "tools": ["Bash"]},
]
```

**2. Error Recovery**

```python
# Built-in error handling in workflow
workflow = {
    "execute": {
        "agent": "ReActAgent",
        "on_success": "verify",
        "on_error": "diagnose",
        "max_retries": 3
    },
    "diagnose": {
        "agent": "PlanningAgent",
        "next": "retry_or_escalate"
    }
}
```

**3. Tool Composition**

```python
# Tools can call other tools
class BrowserUseTool:
    def __init__(self):
        self.search = GoogleSearch()
        self.navigate = WebBrowser()
    
    def execute(self, task):
        # Compose simpler tools
        query = self.search.execute(task)
        result = self.navigate.execute(query.url)
        return result
```

### Key Takeaways for Prompt Engineering

**1. Three-Prompt Pattern**
- **System**: Set persistent context (role, capabilities, constraints)
- **Step**: Guide current action (specific tools, format)
- **Planning**: Enable decomposition (structure, dependencies)

**2. Progressive Specialization**
- Start with base capabilities
- Add layers for specific behaviors
- Each layer enhances, doesn't replace

**3. Graph-Based Orchestration**
- Explicit workflow definition
- Built-in error paths
- Visual representation aids debugging

**4. Containerized Safety**
- Isolated execution prevents side effects
- Resource limits prevent runaway processes
- Easy rollback through container snapshots

**5. Tool Categorization**
- Group by domain, not alphabetically
- Clear purpose for each category
- Makes tool selection easier for agents

### Integration Example: OpenManus + Codex Patterns

```python
# Combine OpenManus hierarchy with Codex prompt structure

# System prompt (OpenManus + Codex role hierarchy)
system_prompt = f"""
system: [Model instructions]
developer: [Framework config - OpenManus workflow]
user: [System prompt - agent role and capabilities]
"""

# Step prompt (OpenManus + Codex XML tags)
step_prompt = f"""
<current_step>
  <objective>{task.objective}</objective>
  <tools>{available_tools}</tools>
  <context>{previous_steps}</context>
</current_step>
"""

# Planning prompt (OpenManus + Claude Code plan mode)
planning_prompt = f"""
Task: {task}

Create a graph-based execution plan with:
1. Nodes: Subtasks with dependencies
2. Edges: Execution flow
3. Error handlers: Recovery paths
4. Success criteria: Per-node validation
"""
```

**Sources:**
- [OpenManus Technical Analysis](https://llmmultiagents.com/en/blogs/OpenManus_Technical_Analysis)
- [OpenManus Architecture Overview](https://albanna-tutorials.com/openmanus.html)
- [OpenManus Official Documentation](https://openmanus.github.io/)
- [OpenManus GitHub Repository](https://github.com/FoundationAgents/OpenManus) - Code analysis

### Code-Level Insights from Implementation

**Studying the actual OpenManus codebase reveals practical patterns:**

#### 1. Agent State Management

```python
# app/agent/base.py
class BaseAgent:
    state: AgentState = AgentState.IDLE  # IDLE, RUNNING, FINISHED, ERROR
    
    @asynccontextmanager
    async def state_context(self, new_state: AgentState):
        """Safe state transitions with automatic rollback"""
        previous_state = self.state
        self.state = new_state
        try:
            yield
        except Exception:
            self.state = AgentState.ERROR  # Auto-transition on failure
        finally:
            self.state = previous_state  # Revert to previous
```

**Key insight:** Use context managers for state transitions. Ensures cleanup and error handling.

#### 2. Stuck State Detection

```python
def is_stuck(self) -> bool:
    """Detect agent loops by counting duplicate assistant messages"""
    if len(self.memory.messages) < 2:
        return False
    
    last_message = self.memory.messages[-1]
    duplicate_count = sum(
        1 for msg in reversed(self.memory.messages[:-1])
        if msg.role == "assistant" and msg.content == last_message.content
    )
    
    return duplicate_count >= self.duplicate_threshold  # Default: 2

def handle_stuck_state(self):
    """Inject prompt to break the loop"""
    stuck_prompt = "Observed duplicate responses. Consider new strategies."
    self.next_step_prompt = f"{stuck_prompt}\n{self.next_step_prompt}"
```

**Critical pattern:** Automatic loop detection + prompt injection to break loops.

#### 3. ReAct Implementation

```python
# app/agent/react.py
class ReActAgent(BaseAgent):
    @abstractmethod
    async def think(self) -> bool:
        """Returns: should_act (bool)"""
    
    @abstractmethod
    async def act(self) -> str:
        """Returns: action result (str)"""
    
    async def step(self) -> str:
        """Execute one cycle"""
        should_act = await self.think()
        if not should_act:
            return "Thinking complete - no action needed"
        return await self.act()
```

**Key insight:** `think()` returns boolean (should we act?), not the action itself. Cleaner separation.

#### 4. Prompt Structure in Code

```python
# app/prompt/manus.py
SYSTEM_PROMPT = (
    "You are OpenManus, an all-capable AI assistant..."
    "The initial directory is: {directory}"  # Variable injection
)

NEXT_STEP_PROMPT = """
Based on user needs, proactively select the most appropriate tool...
After using each tool, clearly explain the execution results and suggest next steps.

If you want to stop, use the `terminate` tool/function call.
"""
```

**Pattern:** System prompts are concise (2-3 sentences). Next-step prompts are longer with instructions.

#### 5. Planning Tool Schema

```python
# app/tool/planning.py
parameters: dict = {
    "properties": {
        "command": {
            "enum": ["create", "update", "list", "get", 
                    "set_active", "mark_step", "delete"]
        },
        "plan_id": {"type": "string"},
        "steps": {"type": "array", "items": {"type": "string"}},
        "step_status": {
            "enum": ["not_started", "in_progress", "completed", "blocked"]
        }
    },
    "required": ["command"]
}
```

**Key insight:** Planning is a CRUD tool (create, read, update, delete plans). Not a separate agent.

#### 6. Tool Collection Pattern

```python
# app/tool/tool_collection.py
class ToolCollection:
    def __init__(self, *tools: BaseTool):
        self.tools = tools
        self.tool_map = {tool.name: tool for tool in tools}
    
    def add_tool(self, tool: BaseTool):
        """Skip if duplicate name (with warning)"""
        if tool.name in self.tool_map:
            logger.warning(f"Tool {tool.name} already exists, skipping")
            return
        self.tools += (tool,)
        self.tool_map[tool.name] = tool
```

**Key insight:** Tools stored in both tuple (order) and dict (fast lookup). Duplicate protection built-in.

#### 7. Manus Agent Composition

```python
# app/agent/manus.py
class Manus(ToolCallAgent):
    system_prompt: str = SYSTEM_PROMPT.format(directory=config.workspace_root)
    next_step_prompt: str = NEXT_STEP_PROMPT
    
    max_observe: int = 10000  # Character limit for observations
    max_steps: int = 20
    
    available_tools: ToolCollection = Field(
        default_factory=lambda: ToolCollection(
            PythonExecute(),
            BrowserUseTool(),
            StrReplaceEditor(),
            AskHuman(),
            Terminate(),
        )
    )
    
    mcp_clients: MCPClients = Field(default_factory=MCPClients)
```

**Pattern:** 
- **Core tools** = built-in (Python, Browser, File, Human, Terminate)
- **MCP tools** = external integrations (added dynamically)

#### 8. Dynamic Tool Loading (MCP)

```python
async def connect_mcp_server(self, server_url: str, server_id: str):
    """Connect and add tools dynamically"""
    await self.mcp_clients.connect_sse(server_url, server_id)
    
    # Add only NEW tools from this server
    new_tools = [
        tool for tool in self.mcp_clients.tools 
        if tool.server_id == server_id
    ]
    self.available_tools.add_tools(*new_tools)

async def disconnect_mcp_server(self, server_id: str):
    """Disconnect and remove tools"""
    await self.mcp_clients.disconnect(server_id)
    
    # Rebuild tools WITHOUT disconnected server's tools
    base_tools = [
        tool for tool in self.available_tools.tools
        if not isinstance(tool, MCPClientTool)
    ]
    self.available_tools = ToolCollection(*base_tools)
    self.available_tools.add_tools(*self.mcp_clients.tools)
```

**Key insight:** MCP tools can be added/removed during runtime. Tools are namespaced by server_id.

#### 9. Browser Context Awareness

```python
# app/agent/manus.py
async def think(self) -> bool:
    """Conditionally add browser context"""
    recent_messages = self.memory.messages[-3:]
    browser_in_use = any(
        tc.function.name == BrowserUseTool().name
        for msg in recent_messages
        if hasattr(msg, 'tool_calls')
    )
    
    if browser_in_use and self.browser_context_helper:
        # Add browser state to prompt
        context = await self.browser_context_helper.get_context()
        self.next_step_prompt = f"{context}\n{original_prompt}"
```

**Key insight:** Context injection is conditional based on recent tool usage. Saves tokens.

#### 10. ToolResult Composition

```python
# app/tool/base.py
class ToolResult(BaseModel):
    output: Any = None
    error: Optional[str] = None
    base64_image: Optional[str] = None
    system: Optional[str] = None
    
    def __add__(self, other: "ToolResult"):
        """Combine multiple tool results"""
        return ToolResult(
            output=self.output + other.output if both else one,
            error=combine(self.error, other.error),
            base64_image=first_non_none,  # Can't concatenate images
            system=combine(self.system, other.system)
        )
```

**Key insight:** Tool results can be combined with `+` operator. Useful for multi-step operations.

#### 11. Memory Management

```python
# app/agent/base.py
def update_memory(self, role: ROLE_TYPE, content: str, **kwargs):
    """Type-safe memory updates"""
    message_map = {
        "user": Message.user_message,
        "system": Message.system_message,
        "assistant": Message.assistant_message,
        "tool": lambda c, **kw: Message.tool_message(c, **kw),
    }
    
    if role not in message_map:
        raise ValueError(f"Unsupported role: {role}")
    
    self.memory.add_message(message_map[role](content, **kwargs))
```

**Pattern:** Factory pattern for message creation. Enforces type safety.

#### 12. Execution Loop with Max Steps

```python
async def run(self, request: Optional[str] = None) -> str:
    """Main execution loop"""
    if request:
        self.update_memory("user", request)
    
    results = []
    async with self.state_context(AgentState.RUNNING):
        while (self.current_step < self.max_steps 
               and self.state != AgentState.FINISHED):
            self.current_step += 1
            logger.info(f"Step {self.current_step}/{self.max_steps}")
            
            step_result = await self.step()
            
            # Check for stuck state
            if self.is_stuck():
                self.handle_stuck_state()
            
            results.append(f"Step {self.current_step}: {step_result}")
        
        if self.current_step >= self.max_steps:
            self.current_step = 0
            self.state = AgentState.IDLE
            results.append(f"Terminated: Max steps ({self.max_steps})")
    
    await SANDBOX_CLIENT.cleanup()
    return "\n".join(results)
```

**Key patterns:**
- State context manager for cleanup
- Stuck detection after each step
- Reset state after max steps
- Sandbox cleanup always happens

### Production Implementation Patterns Summary

**From OpenManus codebase:**

1. **State Management**: Use context managers for safe transitions
2. **Loop Detection**: Count duplicate assistant messages, inject prompt to break
3. **Think vs Act**: `think()` returns bool (should act?), cleaner separation
4. **Prompt Structure**: System (concise) + Next-step (detailed instructions)
5. **Tool Organization**: Tuple (order) + Dict (lookup), duplicate protection
6. **Dynamic Tools**: MCP tools added/removed at runtime, namespaced by server
7. **Conditional Context**: Only inject tool-specific context when tool was recently used
8. **Tool Results**: Composable with `+` operator for multi-step ops
9. **Memory Safety**: Factory pattern for message creation, type enforcement
10. **Execution Loop**: State context + stuck detection + cleanup always happens

**Key differences from documented architecture:**
- Planning is a CRUD tool, not a separate agent layer
- Browser context injected conditionally (token optimization)
- MCP tools can be hot-swapped during execution
- Stuck detection is automatic, not manual
- Tool results are composable objects, not strings

---

## Last Updated

January 24, 2026

## Contributing

This document synthesizes research from academic papers, industry blogs, official documentation, and production systems. All sources are cited inline.

For updates or corrections, please reference specific sections and provide source links.
